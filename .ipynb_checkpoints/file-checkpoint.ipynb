{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Model \n",
    "from keras.layers import Input, Activation, Embedding, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"donaldtweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to Lower case\n",
    "text = df['Tweet_Text'].str.lower()\n",
    "\n",
    "## Remove the URLs\n",
    "text = text.map(lambda s: ' '.join([x for x in s.split() if 'http' not in x]))\n",
    "\n",
    "## Remove short tweets\n",
    "text = text[text.map(len)>40]\n",
    "\n",
    "## Remove emojis\n",
    "text = text.apply(lambda x: x.encode('ascii', 'ignore').decode('ascii'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tokenizer = Tokenizer()\n",
    " tokenizer.fit_on_texts(text)\n",
    " vocab_size = len(tokenizer.word_counts) + 1\n",
    " print(\"Total Vocabulary: \", vocab_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " N = text.shape[0]\n",
    " print(\"Total Row Count: \", N)\n",
    " prop_train = 0.8\n",
    " train = int(N*prop_train)\n",
    " print(\"Training Data Count: \", train)\n",
    " test = N - train\n",
    " print(\"Test Data Coount: \", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, index_train, index_test = [], [], []\n",
    "count = 0\n",
    "for irow,line in enumerate(text):\n",
    "    #print(irow, line)\n",
    "    encoded = tokenizer.texts_to_sequences([line])[0]    \n",
    "    #print(encoded)\n",
    "    for i in range(1, len(encoded)):\n",
    "        sequence = encoded[:i+1]\n",
    "        sequences.append(sequence)\n",
    "        if irow < train:        \n",
    "            index_train.append(count)     \n",
    "        else:         \n",
    "            index_test.append(count)\n",
    "        count += 1\n",
    "print('Total Sequences: %d' % (len(sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "max_length = max([len(seq) for seq in sequences])\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "print('Max Sequence Length: %d' % (max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1],sequences[:,-1]\n",
    "print(y.shape)\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "X_train, y_train, X_test, y_test = X[index_train], y[index_train],X[index_test],  y[index_test]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size,\n",
    "                  input_length=1,\n",
    "                  dim_dense_embedding=10,\n",
    "                  hidden_unit_LSTM=5):\n",
    "    main_input = Input(shape=(input_length,),dtype='int32',name='main_input')\n",
    "    embedding = Embedding(vocab_size, dim_dense_embedding, \n",
    "                          input_length=input_length)(main_input)\n",
    "    x = LSTM(hidden_unit_LSTM)(embedding)\n",
    "    main_output = Dense(vocab_size, activation='softmax')(x)\n",
    "    model = Model(inputs=[main_input],\n",
    "                   outputs=[main_output])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('trump_tweets_generator_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_text = \"America\"\n",
    "for _ in range(50):\n",
    "    # encode the text as integer\n",
    "    enc = tokenizer.texts_to_sequences([in_text])[0]\n",
    "    #print(enc)\n",
    "    # pre-pad sequences to a fixed length\n",
    "    enc_padding = pad_sequences([enc], maxlen=max_length-1, padding='pre')\n",
    "    #print(enc_padding)\n",
    "    probs = model.predict(enc_padding, verbose=0).flatten()\n",
    "    #print(probs)\n",
    "    index = np.random.choice(range(len(probs)),p=probs)\n",
    "    #print(index)\n",
    "    index_word = {v: k for k,v in tokenizer.word_index.items()}\n",
    "    word = index_word[index] \n",
    "    in_text += ' ' + word\n",
    "print(in_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "285ddf65587f3c042e287c3856e7d92f4bb8cbaf58923bfd87e25b328822367e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
